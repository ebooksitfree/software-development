![Cover image for Multi-Armed Bandits](https://imgdetail.ebookreading.net/cover/cover/20200920/EB9781627058711.jpg)

[Multi-Armed Bandits](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_1.html "Multi-Armed Bandits")
====================================================================================================================

Author : [Qing Zhao](https://ebookreading.net/search/author/Qing+Zhao),[ 
            R. Srikant](https://ebookreading.net/search/author/+%0D%0A++++++++++++R.+Srikant)

Release Date : 2019/11/01

Book Description
-----------------


    
    Multi-armed bandit problems pertain to optimal sequential decision making and learning in unknown environments.
Since the first bandit problem posed by Thompson in 1933 for the application of clinical trials, bandit problems have enjoyed lasting attention from multiple research communities and have found a wide range of applications across diverse domains. This book covers classic results and recent development on both Bayesian and frequentist bandit problems. We start in Chapter 1 with a brief overview on the history of bandit problems, contrasting the two schools—Bayesian and frequentis —of approaches and highlighting foundational results and key applications. Chapters 2 and 4 cover, respectively, the canonical Bayesian and frequentist bandit models. In Chapters 3 and 5, we discuss major variants of the canonical bandit models that lead to new directions, bring in new techniques, and broaden the applications of this classical problem. In Chapter 6, we present several representative application examples in communication networks and social-economic systems, aiming to illuminate the connections between the Bayesian and the frequentist formulations of bandit problems and how structural results pertaining to one may be leveraged to obtain solutions under the other.

  

Table of Contents
-----------------

1. [Acknowledgments](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_5.html)
1. [Introduction](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_6.html)
    1. [Multi-Armed Bandit Problems](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_6.html)
    1. [An Essential Conflict: Exploration vs. Exploitation](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_6.html)
    1. [Two Formulations: Bayesian and Frequentist](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_7.html)
        1. [The Bayesian Framework](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_8.html)
        1. [The Frequentist Framework](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_9.html)
    1. [Notation](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_10.html)
1. [Bayesian Bandit Model and Gittins Index](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_11.html)
    1. [Markov Decision Processes](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_11.html)
        1. [Policy and the Value of a Policy](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_11.html)
        1. [Optimality Equation and Dynamic Programming](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_12.html)
    1. [The Bayesian Bandit Model](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_13.html)
    1. [Gittins Index](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_14.html)
        1. [Gittins Index and Forward Induction](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_14.html)
        1. [Interpretations of Gittins Index](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_15.html)
        1. [The Index Process, Lower Envelop, and Monotonicity of the Stopping Sets](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_16.html)
    1. [Optimality of the Gittins Index Policy](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_17.html)
    1. [Computing Gittins Index](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_18.html)
        1. [Offline Computation](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_18.html)
        1. [Online Computation](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_19.html)
    1. [Semi-Markov Bandit Processes](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_19.html)
1. [Variants of the Bayesian Bandit Model](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_20.html)
    1. [Necessary Assumptions for the Index Theorem](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_20.html)
        1. [Modeling Assumptions on the Action Space](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_21.html)
        1. [Modeling Assumptions on the System Dynamics](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_22.html)
        1. [Modeling Assumptions on the Reward Structure](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_23.html)
        1. [Modeling Assumptions on the Performance Measure](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_23.html)
    1. [Variations in the Action Space](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_24.html)
        1. [Multitasking: The Bandit Superprocess Model](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_24.html)
        1. [Bandits with Precedence Constraints](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_25.html)
        1. [Open Bandit Processes](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_26.html)
    1. [Variations in the System Dynamics](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_26.html)
        1. [The Restless Bandit Model](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_26.html)
        1. [Indexability and Whittle Index](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_27.html)
        1. [Optimality of Whittle Index Policy](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_28.html)
        1. [Computational Approaches to Restless Bandits](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_29.html)
    1. [Variations in the Reward Structure](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_29.html)
        1. [Bandits with Rewards under Passivity](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_29.html)
        1. [Bandits with Switching Cost and Switching Delay](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_30.html)
    1. [Variations in Performance Measure](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_31.html)
        1. [Stochastic Shortest Path Bandit](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_31.html)
        1. [Average-Reward and Sensitive-Discount Criteria](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_32.html)
        1. [Finite-Horizon Criterion: Bandits with Deadlines](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_33.html)
1. [Frequentist Bandit Model](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_34.html)
    1. [Basic Formulations and Regret Measures](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_34.html)
        1. [Uniform Dominance vs. Minimax](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_35.html)
        1. [Problem-Specific Regret and Worst-Case Regret](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_36.html)
        1. [Reward Distribution Families and Admissible Policy Classes](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_37.html)
    1. [Lower Bounds on Regret](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_38.html)
        1. [The Problem-Specific Regret](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_38.html)
        1. [The Minimax Regret](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_39.html)
    1. [Online Learning Algorithms](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_40.html)
        1. [Asymptotically Optimal Policies](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_40.html)
        1. [Order-Optimal Policies (1/2)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_41.html)
        1. [Order-Optimal Policies (2/2)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_42.html)
    1. [Connections between Bayesian and Frequentist Bandit Models](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_43.html)
        1. [Frequentist Approaches to Bayesian Bandits](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_43.html)
        1. [Bayesian Approaches to Frequentist Bandits](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_44.html)
1. [Variants of the Frequentist Bandit Model](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_45.html)
    1. [Variations in the Reward Model](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_45.html)
        1. [Rested Markov Reward Processes](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_46.html)
        1. [Restless Markov Reward Processes](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_47.html)
        1. [Nonstationary Reward Processes](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_48.html)
        1. [Nonstochastic Reward Processes: Adversarial Bandits](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_49.html)
    1. [Variations in the Action Space](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_50.html)
        1. [Large-Scale Bandits with Structured Action Space](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_50.html)
        1. [Constrained Action Space](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_51.html)
    1. [Variations in the Observation Model](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_51.html)
        1. [Full-Information Feedback: The Expert Setting](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_51.html)
        1. [Graph-Structured Feedback: Bandits with Side Observations](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_52.html)
        1. [Constrained and Controlled Feedback: Label-Efficient Bandits](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_53.html)
        1. [Comparative Feedback: Dueling Bandits](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_53.html)
    1. [Variations in the Performance Measure](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_54.html)
        1. [Risk-Averse Bandits](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_54.html)
        1. [Pure-Exploration Bandits: Active Inference](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_55.html)
    1. [Learning in Context: Bandits with Side Information](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_56.html)
    1. [Learning under Competition: Bandits with Multiple Players](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_57.html)
        1. [Centralized Learning](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_58.html)
        1. [Distributed Learning](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_59.html)
1. [Application Examples](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_60.html)
    1. [Communication and Computer Networks](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_60.html)
        1. [Dynamic Multichannel Access](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_60.html)
        1. [Adaptive Routing under Unknown Link States](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_61.html)
        1. [Heavy Hitter and Hierarchical Heavy Hitter Detection](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_62.html)
    1. [Social-Economic Networks](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_63.html)
        1. [Dynamic Pricing and the Pursuit of Complete Learning](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_63.html)
        1. [Web Search, Ads Display, and Recommendation Systems: Learning to Rank](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_64.html)
1. [Bibliography (1/4)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_65.html)
1. [Bibliography (2/4)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_66.html)
1. [Bibliography (3/4)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_67.html)
1. [Bibliography (4/4)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_68.html)
1. [Author's Biography](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_69.html)
1. [Blank Page (1/4)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_1.html)
1. [Blank Page (2/4)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_2.html)
1. [Blank Page (3/4)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_3.html)
1. [Blank Page (4/4)](https://ebookreading.net/view/book/Multi-Armed+Bandits-EB9781627058711_4.html)
